{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ec2cdd5",
   "metadata": {},
   "source": [
    "# 自然言語処理入門\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shinchu/dataviz-notebooks/blob/main/week_5/intro-to-nlp.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545f85a0",
   "metadata": {},
   "source": [
    "テキストデータの処理と分析の基礎である自然言語処理を概観しましょう。\n",
    "\n",
    "まず、テキストデータの前処理を段階を踏んで見ていきます。次に、自然言語処理の基本的な考え方とテキストデータのベクトル化を学びます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def2e058",
   "metadata": {},
   "source": [
    "## テキストデータの前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1238e909",
   "metadata": {},
   "source": [
    "テキストデータを分析する際には、基本的に以下の処理が行われます。\n",
    "\n",
    "1. 形態素解析\n",
    "    * 単語分割\n",
    "    * 品詞付与\n",
    "    * 原形抽出\n",
    "2. 構文解析\n",
    "\n",
    "\n",
    "形態素は、「言葉が意味を持つまとまりの単語の最小単位」で、形態素解析は、文章を一つ一つの形態素に分ける技術です。単語が区切られていない日本語などの言語では特に重要です。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c5c429",
   "metadata": {},
   "source": [
    "日本語を例に、`spaCy`と`GiNZA`というライブラリを使って処理の過程を見ていきましょう。\n",
    "\n",
    "`spaCy`では上の一連の処理をまとめて行ってくれます。\n",
    "\n",
    "テキストデータとして、オー・ヘンリー（結城浩訳）[『最後の一枚の葉』](https://www.hyuki.com/trans/leaf.html)の冒頭部分を使います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e459e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "ワシントン・スクエア西にある小地区は、 道路が狂ったように入り組んでおり、 「プレース」と呼ばれる区域に小さく分かれておりました。 この「プレース」は不可思議な角度と曲線を描いており、 一、二回自分自身と交差している通りがあるほどでした。 かつて、ある画家は、この通りが貴重な可能性を持っていることを発見しました。 例えば絵や紙やキャンバスの請求書を手にした取り立て屋を考えてみてください。 取り立て屋は、この道を歩き回ったあげく、 ぐるりと元のところまで戻ってくるに違いありません。 一セントも取り立てることができずにね。\n",
    "それで、芸術家たちはまもなく、奇妙で古いグリニッチ・ヴィレッジへとやってきました。 そして、北向きの窓と十八世紀の切り妻とオランダ風の屋根裏部屋と安い賃貸料を探してうろついたのです。 やがて、彼らは しろめ製のマグやこんろ付き卓上なべを一、二個、六番街から持ち込み、 「コロニー」を形成することになりました。\n",
    "ずんぐりした三階建ての煉瓦造りの最上階では、スーとジョンジーがアトリエを持っていました。 「ジョンジー」はジョアンナの愛称です。 スーはメイン州の、ジョンジーはカリフォルニア州の出身でした。 二人は八番街の「デルモニコの店」の定食で出会い、 芸術と、チコリーのサラダと、ビショップ・スリーブの趣味がぴったりだとわかって、 共同のアトリエを持つことになったのでした。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54829e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリのインストール（初回のみ）\n",
    "\n",
    "!pip install spacy ginza ja-ginza\n",
    "!pip install scikit-learn pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeee17d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリのインポート\n",
    "import spacy\n",
    "\n",
    "# 日本語モデルのロード\n",
    "nlp = spacy.load(\"ja_ginza\")\n",
    "\n",
    "# 解析\n",
    "doc = nlp(text)\n",
    "\n",
    "# 結果の確認\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86492713",
   "metadata": {},
   "source": [
    "形態素解析の結果には、語の原形や品詞の情報も含まれます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b24b559",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(f\"{token}\\t{token.lemma_}\\t{token.pos_}\\t{token.tag_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818bd3e7",
   "metadata": {},
   "source": [
    "構文解析の結果は、次のように確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad67df8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 解析結果をpandasのDataFrameに入れる\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"text\": token.text,\n",
    "    \"lemma_\": token.lemma_,\n",
    "    \"pos_\": token.pos_,\n",
    "    \"tag_\": token.tag_,\n",
    "    \"dep_\": token.dep_,\n",
    "    \"children\": list(token.children)\n",
    "} for token in doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839896a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d99751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 係り受けの図を表示する\n",
    "\n",
    "spacy.displacy.render(doc, style=\"dep\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db53efd",
   "metadata": {},
   "source": [
    "前処理が完了したところで、データの確認として単語の使用頻度を数えてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88206e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 単語の頻度を数える\n",
    "counter = Counter(token.lemma_ for token in doc)\n",
    "\n",
    "# 出現頻度top 20を出力する\n",
    "for word, count in counter.most_common(20):\n",
    "    print(f\"{count:>5} {word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d135cab1",
   "metadata": {},
   "source": [
    "句読点や助詞など、意味がなさそうな言葉ばかりです。\n",
    "\n",
    "より意味がある語を取り出すために、分析対象とする品詞を指定しましょう。具体的には、内容語である名詞、動詞、形容詞（、固有名詞）を指定すればよいでしょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ba7566",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析対象とする品詞の指定\n",
    "include_pos = (\"NOUN\", \"VERB\", \"ADJ\", \"PROPN\")\n",
    "\n",
    "# 再度単語の頻度を数える\n",
    "counter = Counter(token.lemma_ for token in doc if token.pos_ in include_pos)\n",
    "\n",
    "# 出現頻度top 20を出力する\n",
    "for word, count in counter.most_common(20):\n",
    "    print(f\"{count:>5} {word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b897620e",
   "metadata": {},
   "source": [
    "ちょっとよくなりました。でも、「こと」「ある」「いる」などの一般的な名詞や動詞が多いように思えます。\n",
    "\n",
    "これらを不要語として指定し、除去しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b29334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析対象とする品詞と不要語（ストップワード）を指定する\n",
    "include_pos = (\"NOUN\", \"VERB\", \"ADJ\", \"PROPN\")\n",
    "stopwords = (\"する\", \"ある\", \"おる\", \"ない\", \"いう\", \"もの\", \"こと\", \"よう\", \"なる\", \"ほう\", \"いる\", \"くる\")\n",
    "\n",
    "# 再度単語の頻度を数える\n",
    "counter = Counter(token.lemma_ for token in doc\n",
    "                  if token.pos_ in include_pos and token.lemma_ not in stopwords)\n",
    "\n",
    "# 出現頻度top 20を出力する\n",
    "for word, count in counter.most_common(20):\n",
    "    print(f\"{count:>5} {word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84de40a",
   "metadata": {},
   "source": [
    "ずっと良くなりました。\n",
    "\n",
    "これは、語の出現頻度を要素としたテキストデータのベクトル表現で、Bag of Wordsと呼ばれます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf45b50",
   "metadata": {},
   "source": [
    "### tf-idf\n",
    "\n",
    "Bag of Wordsは、「多く出現する語ほど重要である」という直観的な考え方に基づいてテキストデータの特徴を表現したベクトルであり、文書分類を含む多くの自然言語処理タスクに有用です。\n",
    "\n",
    "しかし、いろいろな文書に多く出現すると予測される「わたし」などの言葉は、出現頻度のわりにさほど重要ではないと考えられます。今回の処理のようにこのような言葉を不要語として指定し、分析から除外することもできますが、この点を考慮した単語の重要度の指標として、tf-idfがあります。\n",
    "\n",
    "tf-idfは、term frequency（単語頻度）-inverse document frequency（逆文書頻度）の略です。具体的には、「ある文書内である単語がどれくらい多い頻度で出現するか」を表すterm frequencyと、「全文書内である単語を含む文書がどれくらい少ない頻度で出現するか」を表すinverse document frequencyをかけ合わせた値です。どの文書にもよく出てくる単語の重要度を下げて、あまり出てこない単語の重要度を上げるために工夫された指標です。\n",
    "\n",
    "$$\n",
    "tf-idf = tf（単語頻度） \\times \\frac{N（文書総数）}{n（単語が出現する文書数）}\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "tf-idfはヒューリスティックな指標で理論的な根拠はあまりありませんが、重要語を上手く抽出でき、文書を特徴づけることができることが知られています。\n",
    "\n",
    "そのため、頻度の代わりにtf-idfを要素としてテキストデータをベクトル化することがよく行われています。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a763c12",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2066d40e",
   "metadata": {},
   "source": [
    "## テキストデータのベクトル化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab8ba67",
   "metadata": {},
   "source": [
    "以下の内容は斎藤（2018）『ゼロから作る Deep Learning 2』に基づいています。\n",
    "\n",
    "日本語や英語など、私たちが普段使っている言葉を自然言語（Natural Language）と言います。自然言語処理（Natural Language Processing）とは、自然言語を処理する分野です。\n",
    "\n",
    "自然言語処理の目標は、人の話す言葉をコンピュータに理解させ、私たちにとって役に立つことをコンピュータに行わせることです。\n",
    "私たちの言葉は「文字」によって表現することができます。そして、言葉の意味は「単語」（正確には形態素）によって構成されます。そのため、自然言語をコンピュータに理解させるためには、「単語の意味」を理解させることが重要です。\n",
    "\n",
    "ここでは、単語の意味を数値で表現する手法を学びます。\n",
    "\n",
    "自然言語処理の研究や応用のために目的をもって収集されたテキストデータを「コーパス」と呼びます。WikipediaやGoogle Newsなどのテキストデータや、シェイクスピアや夏目漱石などの作品群もコーパスです。\n",
    "\n",
    "コーパスはテキストデータであり、そこに含まれる文章は人によって書かれたものです。これはつまり、コーパスには自然言語に対する人の知識が含まれているということです。\n",
    "文章の書き方、単語の選び方、単語の意味などがコーパスには含まれています。\n",
    "\n",
    "テキストデータのベクトル化の目的は、人の知識が詰まったコーパスから自動的に効率よく、そのエッセンスを抽出することです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28dd205",
   "metadata": {},
   "source": [
    "## 簡単なコーパスの前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b75ab17",
   "metadata": {},
   "source": [
    "テキストデータを単語に分割し、分割した単語を単語IDのリストへ変換することで、データの前処理を行いましょう。\n",
    "\n",
    "先程モジュールを使って自動で行った単語分割を手動で行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527deca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"You say goodbye and I say hello.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f048a955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 小文字に変換\n",
    "text = text.lower()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a638cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 句点の前にスペースを挿入\n",
    "text = text.replace(\".\", \" .\")\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e857600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文を単語に分割する\n",
    "words = text.split(' ')\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3eb175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 単語のIDと単語の対応表を作る\n",
    "\n",
    "word_to_id = {}\n",
    "id_to_word = {}\n",
    "\n",
    "for word in words:\n",
    "    if word not in word_to_id:\n",
    "        new_id = len(word_to_id)\n",
    "        word_to_id[word] = new_id\n",
    "        id_to_word[new_id] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c89bbe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29090d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8d2a13",
   "metadata": {},
   "source": [
    "この2つの辞書を使えば、単語から単語IDの検索と、単語IDから単語の検索ができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876bd884",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_word[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df6d215",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id[\"i\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cf38f4",
   "metadata": {},
   "source": [
    "最後に、単語のリストを単語IDのリストに変換し、NumPy配列に変換します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146a1d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "corpus = [word_to_id[w] for w in words]\n",
    "corpus = np.array(corpus)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ed646a",
   "metadata": {},
   "source": [
    "これでコーパスの前処理は終了です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56078ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以上の処理を関数として実装する\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = text.replace(\".\", \" .\")\n",
    "    text = text.replace(\",\", \" ,\")\n",
    "    text = text.replace(\"!\", \" !\")\n",
    "    text = text.replace(\"?\", \" ?\")\n",
    "    text = text.replace(\";\", \" ;\")\n",
    "    text = text.replace(\":\", \" :\")\n",
    "    text = text.replace(\"\\\"\", \"\")\n",
    "    text = text.replace(\"\\'\", \"\")\n",
    "    text = text.replace(\"\\n\", \"\")\n",
    "    words = text.split(' ')\n",
    "\n",
    "    word_to_id = {}\n",
    "    id_to_word = {}\n",
    "    for word in words:\n",
    "        if word not in word_to_id:\n",
    "            new_id = len(word_to_id)\n",
    "            word_to_id[word] = new_id\n",
    "            id_to_word[new_id] = word\n",
    "\n",
    "    corpus = np.array([word_to_id[w] for w in words])\n",
    "\n",
    "    return corpus, word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dd7f53",
   "metadata": {},
   "source": [
    "## 単語の分散表現\n",
    "\n",
    "\n",
    "次に、コーパスを使って単語の意味を抽出しましょう。具体的には、単語をベクトルで表すことを目指します。これは、自然言語処理の分野では、単語の分散表現と呼ばれます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41437b8d",
   "metadata": {},
   "source": [
    "単語の分散表現に関する手法は、「単語の意味は、周囲の単語によって形成される」というアイデアに基づいています。これは、分布仮説と呼ばれるものです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a14e5f7",
   "metadata": {},
   "source": [
    "分布仮説では、単語自体には意味がなく、その単語の「コンテキスト（文脈）」によって、単語の意味が形成されると言われています。\n",
    "\n",
    "たしかに、意味的に同じ単語は、同じような文脈で多く出現します。\n",
    "\n",
    "例えば、\n",
    "\n",
    "* I drink beer.\n",
    "* We drink wine.\n",
    "\n",
    "のようにdrinkの近くには飲み物があらわれやすいでしょう。\n",
    "\n",
    "\n",
    "* I guzzle beer.\n",
    "* We guzzle wine.\n",
    "\n",
    "のような文章では、guzzleという単語がdrinkと同じような文脈で使われていることが分かります。そして、guzzleとdrinkが近い意味の単語だということが導けます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e818d28",
   "metadata": {},
   "source": [
    "## 共起行列"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebded83",
   "metadata": {},
   "source": [
    "分布仮説に基づいて、単語をベクトルで表す方法を考えます。\n",
    "\n",
    "素直な方法は、周囲の単語を数えることです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc101a2",
   "metadata": {},
   "source": [
    "さきほど用意したコーパスに含まれるそれぞれの単語について、そのコンテキスト（目当ての単語の周囲）に含まれる単語の頻度を数えていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fefe2cd",
   "metadata": {},
   "source": [
    "例えば、youという単語に着目すると、\n",
    "\n",
    "||you|say|goodbye|and|i|hello|.|\n",
    "|---|---|---|---|---|---|---|---|\n",
    "|you|0|1|0|0|0|0|0|\n",
    "\n",
    "このような表現になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8092ce7b",
   "metadata": {},
   "source": [
    "全ての語に対してこれを数えると、\n",
    "\n",
    "||you|say|goodbye|and|i|hello|.|\n",
    "|---|---|---|---|---|---|---|---|\n",
    "|you|0|1|0|0|0|0|0|\n",
    "|say|1|0|1|0|1|1|0|\n",
    "|goodbye|0|1|0|1|0|0|0|\n",
    "|and|0|0|1|0|1|0|0|\n",
    "|i|0|1|0|1|0|0|0|\n",
    "|hello|0|1|0|0|0|0|1|\n",
    "|.|0|0|0|0|0|1|0|\n",
    "\n",
    "となります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f10b25",
   "metadata": {},
   "source": [
    "これをNumPy配列にすることで、共起行列ができます。この共起行列を使うと、各単語の分散表現が求められます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d2f87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = np.array([\n",
    "    [0, 1, 0, 0, 0, 0, 0],\n",
    "    [1, 0, 1, 0, 1, 1, 0],\n",
    "    [0, 1, 0, 1, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 1, 0, 0],\n",
    "    [0, 1, 0, 1, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0, 0, 1],\n",
    "    [0, 0, 0, 0, 0, 1, 0],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93fc90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# youの分散表現\n",
    "print(id_to_word[0], C[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c000e76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# andの分散表現\n",
    "print(id_to_word[3], C[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304dac3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_co_matrix(corpus, vocab_size, window_size=1):\n",
    "    corpus_size = len(corpus)\n",
    "    co_matrix = np.zeros((vocab_size, vocab_size), dtype=np.int32)\n",
    "\n",
    "    for idx, word_id in enumerate(corpus):\n",
    "        for i in range(1, window_size + 1):\n",
    "            left_idx = idx - i\n",
    "            right_idx = idx + i\n",
    "\n",
    "            if left_idx >= 0:\n",
    "                left_word_id = corpus[left_idx]\n",
    "                co_matrix[word_id, left_word_id] += 1\n",
    "\n",
    "            if right_idx < corpus_size:\n",
    "                right_word_id = corpus[right_idx]\n",
    "                co_matrix[word_id, right_word_id] += 1\n",
    "\n",
    "    return co_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb619b8",
   "metadata": {},
   "source": [
    "単語の分散表現を使うことで、単語の類似度を計算するなど、より高度な処理ができるようになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681d3064",
   "metadata": {},
   "source": [
    "## ベクトル間の類似度\n",
    "\n",
    "単語のベクトル表現の類似度の指標としては、コサイン類似度がよく使われます。コサイン類似度は、$v = (v_1, v_2, v_3, ..., v_n)$と$w = (w_1, w_2, w_3, ..., w_n)$の2つのベクトルがあるとき、次のように定義されます。\n",
    "\n",
    "$$\n",
    "cos(v, w) = \\frac{v \\cdot w}{|v||w|} = \\frac{x_{1}y_{1} + \\cdots + x_{n}y_{n}}{\\sqrt{x_1^2 + \\cdots + x_n^2}\\sqrt{y_1^2 + \\cdots + y_n^2}}\n",
    "$$\n",
    "\n",
    "コサイン類似度は、直感的には「2つのベクトルがどれだけ同じ方向を向いているか」を表します。2つのベクトルが完全に同じ方向を向いているときコサイン類似度は1になり、完全に逆向きだと-1になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978fedae",
   "metadata": {},
   "source": [
    "コサイン類似度を実装すると、次のようになります。ただ、ゼロベクトル（ベクトルの要素がすべて0のベクトル）が引数に入ると「0除算」が発生してしまいます。\n",
    "\n",
    "この問題には、除算を行う際に小さな値（eps）を加算することで対応します。デフォルト値として1e-8（0.00000001）を設定しましたが、これぐらい小さな値であれば、通常は浮動小数点の「丸め誤差」により他の値に吸収され、最終的な計算結果には影響を与えません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f16b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_similarity(x, y, eps=1e-8):\n",
    "    nx = x / np.sqrt(np.sum(x**2) + eps)\n",
    "    ny = y / np.sqrt(np.sum(y**2) + eps)\n",
    "    return np.dot(nx, ny)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e466aa",
   "metadata": {},
   "source": [
    "この関数を用いると、単語ベクトルの類似度を次のように求めることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182100b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"You say goodbye and I say hello.\"\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d3d7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "c0 = C[word_to_id['you']] # youのベクトル\n",
    "c1 = C[word_to_id['i']] # iのベクトル\n",
    "c2 = C[word_to_id['hello']] # helloのベクトル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dfb6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_similarity(c0, c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a7b197",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_similarity(c1, c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eec3ddd",
   "metadata": {},
   "source": [
    "上記結果から、youとiのコサイン類似度は0.70...であり、iとhelloのコサイン類似度は0.49...であることが分かりました。\n",
    "\n",
    "iはhelloよりyouとより近い語であると言えそうです。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d93d06",
   "metadata": {},
   "source": [
    "## 類似単語のランキング表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1337717f",
   "metadata": {},
   "source": [
    "コサイン類似度を使って、便利な関数を実装しましょう。ある単語がクエリとして与えられたときに、そのクエリに対して類似した単語を上位から順に表示する関数です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5156ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar(query, word_to_id, id_to_word, word_matrix, top=5):\n",
    "\n",
    "    # クエリを取り出す\n",
    "    if query not in word_to_id:\n",
    "        print(f\"{query} is not found\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n[query] {query}\")\n",
    "    query_id = word_to_id[query]\n",
    "    query_vec = word_matrix[query_id]\n",
    "\n",
    "    # コサイン類似度の算出\n",
    "    vocab_size = len(id_to_word)\n",
    "    similarity = np.zeros(vocab_size)\n",
    "    for i in range(vocab_size):\n",
    "        similarity[i] = cos_similarity(word_matrix[i], query_vec)\n",
    "\n",
    "    # コサイン類似度の結果から、その値を高い順に出力\n",
    "    count = 0\n",
    "    for i in (-1 * similarity).argsort():\n",
    "        if id_to_word[i] == query:\n",
    "            continue\n",
    "        print(f\"{id_to_word[i]}: {similarity[i]}\")\n",
    "\n",
    "        count += 1\n",
    "        if count >= top:\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329e41b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"You say goodbye and I say hello.\"\n",
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede43306",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar('you', word_to_id, id_to_word, C, top=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f142187",
   "metadata": {},
   "source": [
    "これを使うと、youに近い語が上から順番に出力されます。\n",
    "\n",
    "iとyouは共に人称代名詞なので類似しているのは納得できますが、helloやgoodbyeとの類似度が高いのは不思議です。実はこれはコーパスのサイズが極端に小さいことに起因します。\n",
    "\n",
    "もう少し大きいコーパスを使ってこの関数を試してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bfb7c1",
   "metadata": {},
   "source": [
    "## 少し大きいコーパスで試してみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3556e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "In a little district west of Washington Square the streets have run crazy and broken themselves into small strips called \"places.\" These \"places\" make strange angles and curves. One Street crosses itself a time or two. An artist once discovered a valuable possibility in this street. Suppose a collector with a bill for paints, paper and canvas should, in traversing this route, suddenly meet himself coming back, without a cent having been paid on account!\n",
    "\n",
    "So, to quaint old Greenwich Village the art people soon came prowling, hunting for north windows and eighteenth-century gables and Dutch attics and low rents. Then they imported some pewter mugs and a chafing dish or two from Sixth Avenue, and became a \"colony.\"\n",
    "\n",
    "At the top of a squatty, three-story brick Sue and Johnsy had their studio. \"Johnsy\" was familiar for Joanna. One was from Maine; the other from California. They had met at the table d'hôte of an Eighth Street \"Delmonico's,\" and found their tastes in art, chicory salad and bishop sleeves so congenial that the joint studio resulted.\n",
    "\n",
    "That was in May. In November a cold, unseen stranger, whom the doctors called Pneumonia, stalked about the colony, touching one here and there with his icy fingers. Over on the east side this ravager strode boldly, smiting his victims by scores, but his feet trod slowly through the maze of the narrow and moss-grown \"places.\"\n",
    "\n",
    "Mr. Pneumonia was not what you would call a chivalric old gentleman. A mite of a little woman with blood thinned by California zephyrs was hardly fair game for the red-fisted, short-breathed old duffer. But Johnsy he smote; and she lay, scarcely moving, on her painted iron bedstead, looking through the small Dutch window-panes at the blank side of the next brick house.\n",
    "\n",
    "One morning the busy doctor invited Sue into the hallway with a shaggy, grey eyebrow.\n",
    "\n",
    "\"She has one chance in - let us say, ten,\" he said, as he shook down the mercury in his clinical thermometer. \" And that chance is for her to want to live. This way people have of lining-u on the side of the undertaker makes the entire pharmacopoeia look silly. Your little lady has made up her mind that she's not going to get well. Has she anything on her mind?\"\n",
    "\n",
    "\"She - she wanted to paint the Bay of Naples some day.\" said Sue.\n",
    "\n",
    "\"Paint? - bosh! Has she anything on her mind worth thinking twice - a man for instance?\"\n",
    "\n",
    "\"A man?\" said Sue, with a jew's-harp twang in her voice. \"Is a man worth - but, no, doctor; there is nothing of the kind.\"\n",
    "\n",
    "\"Well, it is the weakness, then,\" said the doctor. \"I will do all that science, so far as it may filter through my efforts, can accomplish. But whenever my patient begins to count the carriages in her funeral procession I subtract 50 per cent from the curative power of medicines. If you will get her to ask one question about the new winter styles in cloak sleeves I will promise you a one-in-five chance for her, instead of one in ten.\"\n",
    "\n",
    "After the doctor had gone Sue went into the workroom and cried a Japanese napkin to a pulp. Then she swaggered into Johnsy's room with her drawing board, whistling ragtime.\n",
    "\n",
    "Johnsy lay, scarcely making a ripple under the bedclothes, with her face toward the window. Sue stopped whistling, thinking she was asleep.\n",
    "\n",
    "She arranged her board and began a pen-and-ink drawing to illustrate a magazine story. Young artists must pave their way to Art by drawing pictures for magazine stories that young authors write to pave their way to Literature.\n",
    "\n",
    "As Sue was sketching a pair of elegant horseshow riding trousers and a monocle of the figure of the hero, an Idaho cowboy, she heard a low sound, several times repeated. She went quickly to the bedside.\n",
    "\n",
    "Johnsy's eyes were open wide. She was looking out the window and counting - counting backward.\n",
    "\n",
    "\"Twelve,\" she said, and little later \"eleven\"; and then \"ten,\" and \"nine\"; and then \"eight\" and \"seven\", almost together.\n",
    "\n",
    "Sue look solicitously out of the window. What was there to count? There was only a bare, dreary yard to be seen, and the blank side of the brick house twenty feet away. An old, old ivy vine, gnarled and decayed at the roots, climbed half way up the brick wall. The cold breath of autumn had stricken its leaves from the vine until its skeleton branches clung, almost bare, to the crumbling bricks.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8131d20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus, word_to_id, id_to_word = preprocess(text)\n",
    "vocab_size = len(word_to_id)\n",
    "C = create_co_matrix(corpus, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbde6dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_similar('say', word_to_id, id_to_word, C, top=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
