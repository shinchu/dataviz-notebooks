{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "523005b4",
   "metadata": {},
   "source": [
    "# テキストデータの可視化入門\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/shinchu/dataviz-notebooks/blob/main/week_5/intro-to-visualizing-text-data.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4748ec8f",
   "metadata": {},
   "source": [
    "今回は、[青空文庫](https://www.aozora.gr.jp/)に収録されている、夏目漱石の[『三四郎』](https://www.aozora.gr.jp/cards/000148/card794.html)という作品を使って、テキストデータの可視化を練習します。\n",
    "\n",
    "自然言語処理のフレームワークである[spaCy](https://spacy.io/)とこれを利用した日本語NLPライブラリである[GiNZA](https://megagonlabs.github.io/ginza/)を使います。\n",
    "\n",
    "![](https://spacy.io/images/pipeline.svg)\n",
    "\n",
    "spaCyでは、各コンポーネント（機能）が順番に適用されるpipeline方式でテキストが処理されます。基本的には、tokenizer（分かち書き）、tagger（品詞付与）、parser（係り受け解析）、ner（固有表現抽出）、lemmatizer（原形抽出）、textcat（文書分類）というコンポーネントが用意されています。\n",
    "\n",
    "`nlp()`を実行するとデフォルトでtokenizer、tagger、parser、ner、lemmatizerが入力文書に適用されます。\n",
    "\n",
    "spaCyとGiNZAと比べて、より長く使われてきたライブラリとして、\n",
    "\n",
    "* [NLTK (Natural Language Toolkit)](https://www.nltk.org/index.html)\n",
    "* [MeCab](https://taku910.github.io/mecab/)（形態素解析） + [CaboCha](https://taku910.github.io/cabocha/)（係り受け解析）\n",
    "* [JUMAN](https://nlp.ist.i.kyoto-u.ac.jp/?JUMAN)（形態素解析） + [KNP](https://nlp.ist.i.kyoto-u.ac.jp/?KNP)（係り受け解析）\n",
    "\n",
    "などがあります。\n",
    "\n",
    "より高度な分析をする時に必要となることがあるので、余力があったらチェックしてみてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942b0f94",
   "metadata": {},
   "source": [
    "それでは、spaCyを使った形態素解析の方法を確認してから、テキストデータの可視化を行いましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079b9c80",
   "metadata": {},
   "source": [
    "## 形態素解析の練習\n",
    "\n",
    "形態素解析は、文章を一つ一つの形態素に分ける技術です。形態素は、「言葉が意味を持つまとまりの単語の最小単位」です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f7f81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリのインストール（初回のみ）\n",
    "\n",
    "!pip install numpy\n",
    "!pip install spacy ginza ja-ginza\n",
    "!pip install scikit-learn pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07ab651",
   "metadata": {},
   "source": [
    "### 日本語\n",
    "\n",
    "spaCyで日本語の形態素解析モデル（`ja_ginza`）をロードして、形態素解析をしてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4567bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"ja_ginza\")\n",
    "text = \"これは文章です。\"\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1ee04f",
   "metadata": {},
   "source": [
    "形態素解析の結果には、語の原形や品詞の情報も含まれます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d5d685",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"うとうととして目がさめると女はいつのまにか、隣のじいさんと話を始めている。このじいさんはたしかに前の前の駅から乗ったいなか者である。発車まぎわに頓狂な声を出して駆け込んで来て、いきなり肌をぬいだと思ったら背中にお灸のあとがいっぱいあったので、三四郎の記憶に残っている。じいさんが汗をふいて、肌を入れて、女の隣に腰をかけたまでよく注意して見ていたくらいである。\"\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(f\"{token}\\t{token.lemma_}\\t{token.pos_}\\t{token.tag_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e5afef",
   "metadata": {},
   "source": [
    "### 英語\n",
    "\n",
    "spaCyでは、モデルを変えるだけで同じ手順で他言語の解析を行うことができます。\n",
    "\n",
    "英語のモデルでも試してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3b858d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 英語のモデルをダウンロードする\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16440c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = \"This is a sentence.\"\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(f\"{token}\\t{token.lemma_}\\t{token.pos_}\\t{token.tag_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec4c56e",
   "metadata": {},
   "source": [
    "## テキストデータの可視化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b865a216",
   "metadata": {},
   "source": [
    "それでは、『三四郎』を題材にテキストデータの可視化を行ってみましょう。\n",
    "\n",
    "* ワードクラウド\n",
    "* 共起ネットワーク\n",
    "* 共起マトリックス\n",
    "\n",
    "の3種類を紹介します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe05fba",
   "metadata": {},
   "source": [
    "### データの用意"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc88a8b0",
   "metadata": {},
   "source": [
    "まずは、青空文庫からテキストデータをダウンロードします。解凍されたファイルの文字コードがShift-JISになっている点に注意しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c75a244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ファイルをダウンロードする\n",
    "!wget https://www.aozora.gr.jp/cards/000148/files/794_ruby_4237.zip\n",
    "# textフォルダ作る\n",
    "!mkdir -p text\n",
    "# ファイルをtextフォルダに解凍\n",
    "!unzip -d text -o 794_ruby_4237.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeca434f",
   "metadata": {},
   "source": [
    "次に、正規表現で青空文庫のルビ、注、アクセントの記号を取り除きます。\n",
    "\n",
    "同時に、文字コードをShift-JISからUTF-8にします。\n",
    "\n",
    "```\n",
    "\n",
    "-------------------------------------------------------\n",
    "【テキスト中に現れる記号について】\n",
    "\n",
    "《》：ルビ\n",
    "（例）頓狂《とんきょう》\n",
    "\n",
    "｜：ルビの付く文字列の始まりを特定する記号\n",
    "（例）福岡県｜京都郡《みやこぐん》\n",
    "\n",
    "［＃］：入力者注　主に外字の説明や、傍点の位置の指定\n",
    "　　　（数字は、JIS X 0213の面区点番号またはUnicode、底本のページと行数）\n",
    "（例）※［＃「魚＋師のつくり」、第4水準2-93-37］\n",
    "\n",
    "〔〕：アクセント分解された欧文をかこむ\n",
    "（例）〔ve'rite'《ヴェリテ》 vraie《ヴレイ》.〕\n",
    "アクセント分解についての詳細は下記URLを参照してください\n",
    "http://www.aozora.gr.jp/accent_separation.html\n",
    "-------------------------------------------------------\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be8d4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "input_fn = \"text/sanshiro.txt\"\n",
    "output_fn = \"text/sanshiro.stripruby.txt\"\n",
    "\n",
    "with open(input_fn, encoding=\"shift_jis\") as fin, open(output_fn, mode=\"w\") as fout:\n",
    "    for line in fin:\n",
    "        fout.write(re.sub(r\"《[^》]+》|［[^］]+］|〔[^〕]+〕| [｜]\", \"\", line))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94129c7b",
   "metadata": {},
   "source": [
    "冒頭と末尾の説明を取り除きます（何行取り除くかは目視で確認）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a30e5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# macOSの場合は、以下のコマンドでhomebrewからcoreutilsをインストールする必要があります（gheadを使うため）\n",
    "# brew install coreutils\n",
    "# homebrewがインストールされていない場合は、https://brew.sh/からインストールしてください\n",
    "\n",
    "if sys.platform == \"darwin\":\n",
    "    !tail -n +22 text/sanshiro.stripruby.txt | ghead -n -14 > text/sanshiro.corpus.txt\n",
    "else:\n",
    "    !tail -n +22 text/sanshiro.stripruby.txt | head -n -14 > text/sanshiro.corpus.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df3b89d",
   "metadata": {},
   "source": [
    "これで、テキストファイルを扱う準備ができました。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5840589",
   "metadata": {},
   "source": [
    "### ワードクラウド\n",
    "\n",
    "ワードクラウドは、形態素解析で得られた頻出単語の頻出度合いを文字の大きさで可視化する手法です。頻度の高い単語を大きく表示することで、テキスト全体の傾向を素早く理解することができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a55698",
   "metadata": {},
   "source": [
    "#### 形態素解析"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abeb0eee",
   "metadata": {},
   "source": [
    "それでは、『三四郎』に出現する単語の頻度を数えてみましょう。\n",
    "\n",
    "テキストファイルを読み込んで形態素解析を行います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bc7fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fn = \"text/sanshiro.corpus.txt\"\n",
    "output_fn = \"text/sanshiro.wakati.txt\"\n",
    "\n",
    "nlp = spacy.load(\"ja_ginza\")\n",
    "\n",
    "with open(input_fn, \"r\", encoding=\"utf-8\") as fin, \\\n",
    "     open(output_fn, \"w\", encoding=\"utf-8\") as fout:\n",
    "    for line in fin:\n",
    "        tokens = [token.text for token in nlp(line.rstrip())]\n",
    "        fout.write(' '.join(tokens) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0606bfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_in_batches(input_file, output_file, nlp, batch_size=100):\n",
    "    all_tokens = []\n",
    "    lines = []\n",
    "    count = 0\n",
    "\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as fin, \\\n",
    "         open(output_file, \"w\", encoding=\"utf-8\") as fout:\n",
    "\n",
    "        for line in fin:\n",
    "            if len(line.strip()) == 0:  # 空行はスキップ\n",
    "                continue\n",
    "\n",
    "            lines.append(line.rstrip())\n",
    "            count += 1\n",
    "\n",
    "            # バッチサイズに達したら処理\n",
    "            if count % batch_size == 0:\n",
    "                docs = nlp.pipe(lines)\n",
    "                for doc in docs:\n",
    "                    tokens = [token.text for token in doc]\n",
    "                    fout.write(' '.join(tokens) + \"\\n\")\n",
    "                    all_tokens.extend([token for token in doc])\n",
    "                lines = []  # リストをクリア\n",
    "\n",
    "        # 残りの行を処理\n",
    "        if lines:\n",
    "            docs = nlp.pipe(lines)\n",
    "            for doc in docs:\n",
    "                tokens = [token.text for token in doc]\n",
    "                fout.write(' '.join(tokens) + \"\\n\")\n",
    "                all_tokens.extend([token for token in doc])\n",
    "\n",
    "    return all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66456e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 実行\n",
    "nlp = spacy.load(\"ja_ginza\")\n",
    "all_tokens = process_in_batches(input_fn, output_fn, nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd5a14f",
   "metadata": {},
   "source": [
    "出力されたファイルを確認すると、分かち書きされていることが分かります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396e04dc",
   "metadata": {},
   "source": [
    "次に、使用頻度の高い単語を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35165aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 分析対象とする品詞（内容語 - 名詞、動詞、形容詞）と不要語（ストップワード）を指定する\n",
    "include_pos = (\"NOUN\", \"VERB\", \"ADJ\")\n",
    "stopwords = (\"する\", \"ある\", \"ない\", \"いう\", \"もの\", \"こと\", \"よう\", \"なる\", \"ほう\", \"いる\", \"くる\", \"さん\")\n",
    "\n",
    "# 単語の頻度を数える\n",
    "counter = Counter(token.lemma_ for token in all_tokens if token.pos_ in include_pos and token.lemma_ not in stopwords)\n",
    "\n",
    "# 出現頻度top 20を出力する\n",
    "for word, count in counter.most_common(20):\n",
    "    print(f\"{count:>5} {word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b152e15c",
   "metadata": {},
   "source": [
    "#### ワードクラウドの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b9e8fb",
   "metadata": {},
   "source": [
    "それでは、このデータをもとにワードクラウドを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f22e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリをインストールする\n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ca7261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 日本語フォントのダウンロード\n",
    "%%bash\n",
    "sudo apt-get update\n",
    "sudo apt-get install fonts-ipaexfont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6230c37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# フォントファイルの場所の指定（図が上手く表示されない場合は、書き換えてください）\n",
    "import sys\n",
    "\n",
    "if sys.platform == \"darwin\": # macOSの場合\n",
    "    fpath = \"/Library/Fonts/Arial Unicode.ttf\"\n",
    "elif sys.platform == \"linux\": # Linuxの場合\n",
    "    fpath = \"/usr/share/fonts/opentype/ipaexfont-gothic/ipaexg.ttf\"\n",
    "else:\n",
    "    fpath = \"C:\\\\Windows\\\\Fonts\\\\BIZ-UDGothicR.ttc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38faa10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [token.lemma_ for token in all_tokens if token.pos_ in include_pos and token.lemma_ not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7455faec",
   "metadata": {},
   "outputs": [],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98a1286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# ワードクラウドの表示設定と作成\n",
    "wordcloud = WordCloud(\n",
    "    width=1600, height=800,\n",
    "    background_color=\"white\", font_path=fpath\n",
    ").generate(' '.join(words))\n",
    "\n",
    "# プロット\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.savefig(\"wordcloud.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599b22f7",
   "metadata": {},
   "source": [
    "「先生」、「言う」、「女」、「見る」などが高頻度で出現していることが分かります。\n",
    "\n",
    "分析対象の品詞に、固有名詞を加えてみたらどうでしょうか？試してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d298b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析対象とする品詞と不要語を指定する\n",
    "# your code goes in ????? below\n",
    "\n",
    "include_pos = (\"NOUN\", \"VERB\", \"ADJ\", \"?????\")\n",
    "stopwords = (\"する\", \"ある\", \"ない\", \"いう\", \"もの\", \"こと\", \"よう\", \"なる\", \"ほう\", \"いる\", \"くる\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ca2a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [token.lemma_ for token in all_tokens if token.pos_ in include_pos and token.lemma_ not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ad7e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ワードクラウドの表示設定と作成\n",
    "wordcloud = WordCloud(\n",
    "    width=1600, height=800,\n",
    "    background_color=\"white\", font_path=fpath\n",
    ").generate(' '.join(words))\n",
    "\n",
    "# プロット\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d2909c",
   "metadata": {},
   "source": [
    "### 共起ネットワーク\n",
    "\n",
    "次に、共起ネットワークを作って、どの語とどの語が一緒に使われているかを調べてみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b27f9a",
   "metadata": {},
   "source": [
    "#### 共起分析\n",
    "\n",
    "まず、共起分析を行います。文章を文に分割し、同一文中に同時に出現する単語の組を数え上げることで分析します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc555471",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1248d0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_words(sent, pos_tags, stopwords):\n",
    "    \"\"\"\n",
    "    分析対象の品詞であり、不要語ではない単語を抽出する\n",
    "    \"\"\"\n",
    "    if len(pos_tags) == 0:\n",
    "        words = [token.lemma_ for token in sent if token.lemma_ not in stopwords]\n",
    "    else:\n",
    "        words = [token.lemma_ for token in sent if token.pos_ in pos_tags and token.lemma_ not in stopwords]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f96dfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_cooccurrence(sents, token_length=\"{2,}\"):\n",
    "    \"\"\"\n",
    "    同じ文中に共起する単語を行列形式で列挙する\n",
    "    \"\"\"\n",
    "    token_pattern = f\"\\\\b\\\\w{token_length}\\\\b\"\n",
    "    count_model = CountVectorizer(token_pattern=token_pattern)\n",
    "\n",
    "    X = count_model.fit_transform(sents)\n",
    "    words = count_model.get_feature_names_out()\n",
    "    word_counts = np.asarray(X.sum(axis=0)).reshape(-1)\n",
    "\n",
    "    X[X > 0] = 1 # 同じ共起が2以上出現しても1とする\n",
    "    Xc = (X.T * X) # 共起行列を求めるための掛け算をする、csr形式の疎行列\n",
    "\n",
    "    return words, word_counts, Xc, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cfc975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sentence_by_cooccurrence(X, idxs):\n",
    "    \"\"\"\n",
    "    指定された共起を含む文を見つける\n",
    "    \"\"\"\n",
    "    occur_flags = (X[:, idxs[0]] > 0)\n",
    "    for idx in idxs[1:]:\n",
    "        occur_flags = occur_flags.multiply(X[:, idx] > 0)\n",
    "\n",
    "    return occur_flags.nonzero()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becb033e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sentences_in_batches(input_file, nlp, include_pos=[], stopwords=[], batch_size=100, is_lemmatized=True):\n",
    "    sents = []\n",
    "    lines = []\n",
    "    count = 0\n",
    "\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if len(line.strip()) == 0:  # 空行はスキップ\n",
    "                continue\n",
    "\n",
    "            lines.append(line.rstrip())\n",
    "            count += 1\n",
    "\n",
    "            # バッチサイズに達したら処理\n",
    "            if count % batch_size == 0:\n",
    "                docs = nlp.pipe(lines)\n",
    "                for doc in docs:\n",
    "                    if is_lemmatized:\n",
    "                        batch_sents = [' '.join(extract_words(sent, include_pos, stopwords))\n",
    "                                    for sent in doc.sents]\n",
    "                        sents.extend(batch_sents)\n",
    "                    else:\n",
    "                        sents.extend(list(doc.sents))\n",
    "                lines = []  # リストをクリア\n",
    "\n",
    "        # 残りの行を処理\n",
    "        if lines:\n",
    "            docs = nlp.pipe(lines)\n",
    "            for doc in docs:\n",
    "                if is_lemmatized:\n",
    "                    batch_sents = [' '.join(extract_words(sent, include_pos, stopwords))\n",
    "                                for sent in doc.sents]\n",
    "                    sents.extend(batch_sents)\n",
    "                else:\n",
    "                    sents.extend(list(doc.sents))\n",
    "    return sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557a60ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文章を解析し、共起を求める\n",
    "include_pos = (\"NOUN\", \"VERB\", \"ADJ\", \"PROPN\")\n",
    "stopwords = (\"する\", \"ある\", \"ない\", \"いう\", \"もの\", \"こと\", \"よう\", \"なる\", \"ほう\", \"いる\", \"くる\", \"さん\")\n",
    "\n",
    "sents = process_sentences_in_batches(input_fn, nlp, include_pos, stopwords)\n",
    "words, word_counts, Xc, X = count_cooccurrence(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4b8ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 共起ランキングを出力する\n",
    "# 共起行列Xcは疎行列なので、非ゼロ要素のみをカウンタに格納する\n",
    "counter = Counter()\n",
    "for i, j in zip(*Xc.nonzero()):\n",
    "    if i >= j:\n",
    "        continue\n",
    "    counter[(i, j)] += Xc[i, j]\n",
    "\n",
    "# 共起の出現頻度top 20を出力する\n",
    "for (i, j), c in counter.most_common(20):\n",
    "    print(f\"{c:>3d} ({words[i]}, {words[j]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b23b1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定した共起を含む文のリストを出力する（include_posとstopwordsを指定しない）\n",
    "sents_orig = process_sentences_in_batches(input_fn, nlp, batch_size=1, is_lemmatized=False)\n",
    "\n",
    "# すべての単語の通し番号を求める\n",
    "words_lookup = { word: index for index, word in enumerate(words) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde1d4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 共起語を指定する\n",
    "lookup_words = [\"野々宮\", \"美禰子\"]\n",
    "\n",
    "# 指定した共起語のインデックスを求める\n",
    "idxs = list(map(lambda x: words_lookup[x], lookup_words))\n",
    "\n",
    "# 指定した共起を含む文のリストを出力する\n",
    "for i in find_sentence_by_cooccurrence(X, idxs):\n",
    "    print(f\"{i:>5d}: {sents_orig[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8bef69",
   "metadata": {},
   "source": [
    "#### 共起ネットワークの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01164062",
   "metadata": {},
   "source": [
    "共起分析の結果に基づいて共起ネットワークを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790e9ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install networkx pyvis japanize_matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af921ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "import japanize_matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35721025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_weights(words, word_counts):\n",
    "    \"\"\"\n",
    "    単語の最多頻度が1となるような相対値として単語の重みを求める\n",
    "    \"\"\"\n",
    "    count_max = word_counts.max()\n",
    "    weights = [(word, {\"weight\": count / count_max})\n",
    "               for word, count in zip(words, word_counts)]\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722233eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cooccurrence_weights(words, Xc, weight_cutoff):\n",
    "    \"\"\"\n",
    "    共起の最多頻度が1となるような相対値として共起の重みを求める\n",
    "    共起の重みがweight_cutoffより低い共起は除外する\n",
    "    \"\"\"\n",
    "    Xc_max = Xc.max()\n",
    "    cutoff = weight_cutoff * Xc_max\n",
    "    weights = [(words[i], words[j], Xc[i, j] / Xc_max)\n",
    "               for i, j in zip(*Xc.nonzero()) if i < j and Xc[i, j] > cutoff]\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b0afbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network(words, word_counts, Xc, weight_cutoff):\n",
    "    \"\"\"\n",
    "    語、単語頻度、共起行列から共起ネットワークをNetworkX形式で得る\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "\n",
    "    weights_w = word_weights(words, word_counts)\n",
    "    G.add_nodes_from(weights_w)\n",
    "\n",
    "    weights_c = cooccurrence_weights(words, Xc, weight_cutoff)\n",
    "    G.add_weighted_edges_from(weights_c)\n",
    "\n",
    "    G.remove_nodes_from(list(nx.isolates(G)))\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca6c99f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyplot_network(G, font):\n",
    "    \"\"\"\n",
    "    NetworkX形式で与えられた共起ネットワークをpyplotで描画する\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    pos = nx.spring_layout(G, k=0.1)\n",
    "\n",
    "    weights_n = np.array(list(nx.get_node_attributes(G, \"weight\").values()))\n",
    "    nx.draw_networkx_nodes(G, pos, node_size=300 * weights_n)\n",
    "    weights_e = np.array(list(nx.get_edge_attributes(G, \"weight\").values()))\n",
    "    nx.draw_networkx_edges(G, pos, width=20 * weights_e)\n",
    "\n",
    "    nx.draw_networkx_labels(G, pos, font_family=font)\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5066c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nx2pyvis_G(G):\n",
    "    \"\"\"\n",
    "    NetworkX形式で与えられた共起ネットワークをpyvisで描画する\n",
    "    \"\"\"\n",
    "    pyvis_G = Network(width=\"800px\", height=\"800px\", notebook=True)\n",
    "    for node, attrs in G.nodes(data=True):\n",
    "        pyvis_G.add_node(node, title=node, size=30 * attrs[\"weight\"])\n",
    "    for node1, node2, attrs in G.edges(data=True):\n",
    "        pyvis_G.add_edge(node1, node2, width=20 * attrs[\"weight\"])\n",
    "\n",
    "    return pyvis_G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c06913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ネットワークを作る\n",
    "G = create_network(words, word_counts, Xc, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344717cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# フォントの指定（図が上手く表示されない場合は、書き換えてください）\n",
    "\n",
    "import sys\n",
    "\n",
    "if sys.platform == \"darwin\": # macOSの場合\n",
    "    font = \"YuGothic\"\n",
    "elif sys.platform == \"linux\": # Linuxの場合\n",
    "    font = \"IPAexGothic\"\n",
    "else: # その他のOSの場合\n",
    "    font = \"MS Gothic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca19a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 静的ビジュアライゼーション\n",
    "pyplot_network(G, font=font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ef3fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# インタラクティブなビジュアライゼーション\n",
    "pyvis_G = nx2pyvis_G(G)\n",
    "pyvis_G.show_buttons()\n",
    "pyvis_G.show(\"network.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28da92e0",
   "metadata": {},
   "source": [
    "### 共起ヒートマップ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf64940",
   "metadata": {},
   "source": [
    "共起関係をネットワークではなく、ヒートマップとして表現することもできます。\n",
    "\n",
    "登場人物の共起ヒートマップを作ってみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce90a36",
   "metadata": {},
   "source": [
    "#### データフレームを作る"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf4901e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文章を解析し、共起を求める\n",
    "include_pos = (\"NOUN\", \"PROPN\")\n",
    "stopwords = (\"する\", \"ある\", \"ない\", \"いう\", \"もの\", \"こと\", \"よう\", \"なる\", \"ほう\", \"いる\", \"くる\", \"さん\")\n",
    "\n",
    "sents = process_sentences_in_batches(input_fn, nlp, include_pos, stopwords)\n",
    "words, word_counts, Xc, X = count_cooccurrence(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46039f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 共起行列Xcは疎行列なので、非ゼロ要素のみをカウンタに格納する\n",
    "counter = Counter()\n",
    "for i, j in zip(*Xc.nonzero()):\n",
    "    if i >= j:\n",
    "        continue\n",
    "    counter[(i, j)] += Xc[i, j]\n",
    "\n",
    "# 共起の出現頻度top 20を出力する\n",
    "for (i, j), c in counter.most_common(20):\n",
    "    print(f\"{c:>3d} ({words[i]}, {words[j]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51d4dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 行名を用意する\n",
    "\n",
    "columns = set(Xc.nonzero()[0])\n",
    "columns_text = [words[i] for i in columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42569a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# すべて0のデータフレームを用意する\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(np.zeros((len(columns), len(columns))), index=columns_text, columns=columns_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f488da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# データフレームに頻度を入れる\n",
    "\n",
    "for cord, count in counter.items():\n",
    "    df.iloc[cord] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865a9c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 登場人物名のみを取り出す\n",
    "\n",
    "characters = [\"三四郎\", \"広田\", \"野々宮\", \"佐々木\", \"与次郎\", \"美禰子\", \"先生\", \"原口\", \"里見\"]\n",
    "df_characters = df[characters].filter(items=characters, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c693323",
   "metadata": {},
   "source": [
    "#### 共起ヒートマップの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695614a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install plotly kaleido\n",
    "!pip install --upgrade nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e752f2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.imshow(df_characters, color_continuous_scale=px.colors.sequential.Oranges,\n",
    "                title=\"『三四郎』の登場人物\", width=800, height=800)\n",
    "fig.update_layout(font=dict(size=16))\n",
    "fig.show()\n",
    "fig.write_image(\"heatmap.png\", scale=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cbca9a",
   "metadata": {},
   "source": [
    "人々のインタラクションの多寡が示されました。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ca8e60",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0303ca15",
   "metadata": {},
   "source": [
    "これらの可視化手法は文学作品だけではなく、他の分野のテキストデータにも有効です。\n",
    "\n",
    "文学作品に限っていうと、複数の作家の作品群を比較すると面白い結果が出るかもしれません。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
